{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 딥러닝 아키텍처 one-hot-encoding\n",
    "\n",
    "# 입력층 노드를 입력 데이터 개수와 일치하도록 784개 설정\n",
    "# 은닉층 노드를 몇 개로 설정할 것인가는 정해진 규칙이 없으므로 100개 임의 설정 -> 최적의 개수 찾는 알고리즘 필요\n",
    "# 출력층 노드는 10개 설정? 0~9 중 하나의 숫자이므로 리스트에서 가장 큰 값을 가지는 인덱스를 정답으로 판단할 수 있도록 출력 노드 10개로 설정: one-hot-encoding\n",
    "\n",
    "# NeuralNetwork class\n",
    "\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "# 수치미분 함수\n",
    "\n",
    "def numerical_derivative(f, x):\n",
    "    delta_x = 1e-4 # 0.0001\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    \n",
    "    while not it.finished:\n",
    "        idx = it.multi_index        \n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = float(tmp_val) + delta_x\n",
    "        fx1 = f(x) # f(x+delta_x)\n",
    "        \n",
    "        x[idx] = tmp_val - delta_x \n",
    "        fx2 = f(x) # f(x-delta_x)\n",
    "        grad[idx] = (fx1 - fx2) / (2*delta_x)\n",
    "        \n",
    "        x[idx] = tmp_val \n",
    "        it.iternext()   \n",
    "        \n",
    "    return grad\n",
    "\n",
    "# sigmoid 함수\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1+np.exp(-x))\n",
    "\n",
    "# MNIST_Test Class\n",
    "\n",
    "class MNIST_Test:\n",
    "    \n",
    "    # 생성자\n",
    "    # xdata, tdata => numpy.array(...)\n",
    "    def __init__(self, input_nodes, hidden_nodes, output_nodes, learning_rate):\n",
    "        \n",
    "        self.input_nodes = input_nodes # input_nodes = 784\n",
    "        self.hidden_nodes = hidden_nodes # hidden_nodes = 100\n",
    "        self.output_nodes = output_nodes # output_nodes = 10\n",
    "        \n",
    "        # 은닉층 가중치  W2  Xavier/He 방법으로 self.W2 가중치 초기화\n",
    "        self.W2 = np.random.randn(self.input_nodes, self.hidden_nodes) / np.sqrt(self.input_nodes/2)\n",
    "        self.b2 = np.random.rand(self.hidden_nodes)      \n",
    "        \n",
    "        # 출력층 가중치는 W3  Xavier/He 방법으로 self.W3 가중치 초기화\n",
    "        self.W3 = np.random.randn(self.hidden_nodes, self.output_nodes) / np.sqrt(self.hidden_nodes/2)\n",
    "        self.b3 = np.random.rand(self.output_nodes)      \n",
    "        \n",
    "        # 2층 hidden layer unit \n",
    "        # 가중치 W, 바이어스 b 초기화\n",
    "        #self.W2 = np.random.rand(input_nodes, hidden_nodes)  \n",
    "        #self.b2 = np.random.rand(hidden_nodes)\n",
    "        \n",
    "        # 3층 output layer unit : 1 개 \n",
    "        #self.W3 = np.random.rand(hidden_nodes,output_nodes)\n",
    "        #self.b3 = np.random.rand(output_nodes)\n",
    "                        \n",
    "        # 학습률 learning rate 초기화\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        print(\"MNIST_Test object is created !!!\")\n",
    "        \n",
    "    # 손실함수\n",
    "    def feed_forward(self):\n",
    "        \n",
    "        delta = 1e-7    # log 무한대 발산 방지\n",
    "    \n",
    "        z1 = np.dot(self.input_data, self.W2) + self.b2\n",
    "        y1 = sigmoid(z1)\n",
    "        \n",
    "        z2 = np.dot(y1, self.W3) + self.b3\n",
    "        y = sigmoid(z2)\n",
    "    \n",
    "        # cross-entropy \n",
    "        return  -np.sum( self.target_data*np.log(y + delta) + (1-self.target_data)*np.log((1 - y)+delta ) )\n",
    "    \n",
    "    # obtain W and b\n",
    "    def get_W_b(self):\n",
    "        \n",
    "        return self.W2,  self.b2, self.W3, self.b3\n",
    "    \n",
    "    # 손실 값 계산\n",
    "    def loss_val(self):\n",
    "        \n",
    "        delta = 1e-7    # log 무한대 발산 방지\n",
    "    \n",
    "        z1 = np.dot(self.input_data, self.W2) + self.b2\n",
    "        y1 = sigmoid(z1)\n",
    "        \n",
    "        z2 = np.dot(y1, self.W3) + self.b3\n",
    "        y = sigmoid(z2)\n",
    "    \n",
    "        # cross-entropy \n",
    "        return  -np.sum( self.target_data*np.log(y + delta) + (1-self.target_data)*np.log((1 - y)+delta ) )\n",
    "    \n",
    "    # query, 즉 미래 값 예측 함수\n",
    "    def predict(self, input_data):    \n",
    "        \n",
    "        z2 = np.dot(input_data, self.W2) + self.b2\n",
    "        a2 = sigmoid(z2)\n",
    "        \n",
    "        z3 = np.dot(a2, self.W3) + self.b3\n",
    "        y = a3 = sigmoid(z3)\n",
    "    \n",
    "        # MNIST 경우는 one-hot encoding 을 적용하기 때문에\n",
    "        # 0 또는 1 이 아닌 argmax() 를 통해 최대 인덱스를 넘겨주어야 함\n",
    "        predicted_num = np.argmax(y)\n",
    "    \n",
    "        return predicted_num\n",
    "\n",
    "    # 정확도 측정함수\n",
    "    def accuracy(self, input_data, target_data):\n",
    "        \n",
    "        matched_list = []\n",
    "        not_matched_list = []\n",
    "        \n",
    "        # list which contains (index, label, prediction) value\n",
    "        index_label_prediction_list = []\n",
    "        \n",
    "        # temp list which contains label and prediction in sequence\n",
    "        temp_list = []\n",
    "        \n",
    "        for index in range(len(input_data)):\n",
    "                        \n",
    "            label = int(target_data[index])\n",
    "                        \n",
    "            # normalize\n",
    "            data = (input_data[index, :] / 255.0 * 0.99) + 0.01\n",
    "      \n",
    "            predicted_num = self.predict(data)\n",
    "        \n",
    "            if label == predicted_num:\n",
    "                matched_list.append(index)\n",
    "                \n",
    "            else:\n",
    "                not_matched_list.append(index)\n",
    "                \n",
    "                temp_list.append(index)\n",
    "                temp_list.append(label)\n",
    "                temp_list.append(predicted_num)\n",
    "                \n",
    "                index_label_prediction_list.append(temp_list)\n",
    "                \n",
    "                temp_list = []\n",
    "                \n",
    "        print(\"Current Accuracy = \", len(matched_list)/(len(input_data)) )\n",
    "        \n",
    "        return matched_list, not_matched_list, index_label_prediction_list\n",
    "    \n",
    "        \n",
    "    # 수치미분을 이용하여 손실함수가 최소가 될때 까지 학습하는 함수\n",
    "    def train(self, input_data, target_data):\n",
    "        \n",
    "        self.input_data = input_data\n",
    "        self.target_data = target_data\n",
    "        \n",
    "        f = lambda x : self.feed_forward()\n",
    "        \n",
    "        self.W2 -= self.learning_rate * numerical_derivative(f, self.W2)\n",
    "    \n",
    "        self.b2 -= self.learning_rate * numerical_derivative(f, self.b2)\n",
    "        \n",
    "        self.W3 -= self.learning_rate * numerical_derivative(f, self.W3)\n",
    "    \n",
    "        self.b3 -= self.learning_rate * numerical_derivative(f, self.b3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training_data.shape =  (60000, 785)\n",
      "MNIST_Test object is created !!!\n",
      "Neural Network Learning using Numerical Derivative...\n",
      "epochs =  0 , index =  0 , loss value =  9.009995183389151\n",
      "epochs =  0 , index =  200 , loss value =  3.249003671046065\n",
      "epochs =  0 , index =  400 , loss value =  2.749982169335703\n",
      "epochs =  0 , index =  600 , loss value =  2.966049241744029\n",
      "epochs =  0 , index =  800 , loss value =  2.72305867291746\n",
      "epochs =  0 , index =  1000 , loss value =  2.4351034833542746\n",
      "epochs =  0 , index =  1200 , loss value =  1.8818115398544002\n",
      "epochs =  0 , index =  1400 , loss value =  1.9371458437505575\n",
      "epochs =  0 , index =  1600 , loss value =  2.9574826897234288\n",
      "epochs =  0 , index =  1800 , loss value =  2.294415329091309\n",
      "epochs =  0 , index =  2000 , loss value =  3.3264463353587934\n",
      "epochs =  0 , index =  2200 , loss value =  2.204253946563692\n",
      "epochs =  0 , index =  2400 , loss value =  1.86612459361987\n",
      "epochs =  0 , index =  2600 , loss value =  2.546647224048486\n",
      "epochs =  0 , index =  2800 , loss value =  2.1402039241716606\n",
      "epochs =  0 , index =  3000 , loss value =  3.0499535819313635\n",
      "epochs =  0 , index =  3200 , loss value =  2.2049644137421796\n",
      "epochs =  0 , index =  3400 , loss value =  1.3657336096764947\n",
      "epochs =  0 , index =  3600 , loss value =  2.2152504920063136\n",
      "epochs =  0 , index =  3800 , loss value =  1.1385255048268514\n",
      "epochs =  0 , index =  4000 , loss value =  1.3502381737866755\n",
      "epochs =  0 , index =  4200 , loss value =  1.6429982750838201\n",
      "epochs =  0 , index =  4400 , loss value =  1.1471392806393454\n",
      "epochs =  0 , index =  4600 , loss value =  2.611273515253018\n",
      "epochs =  0 , index =  4800 , loss value =  2.1517998087199883\n",
      "epochs =  0 , index =  5000 , loss value =  1.103521528165524\n",
      "epochs =  0 , index =  5200 , loss value =  2.028108985349369\n",
      "epochs =  0 , index =  5400 , loss value =  2.0508562358574194\n",
      "epochs =  0 , index =  5600 , loss value =  1.5447138269489473\n",
      "epochs =  0 , index =  5800 , loss value =  2.9696397294576404\n",
      "epochs =  0 , index =  6000 , loss value =  0.7438978025822479\n",
      "epochs =  0 , index =  6200 , loss value =  2.2086798738694466\n",
      "epochs =  0 , index =  6400 , loss value =  0.7875946876575968\n",
      "epochs =  0 , index =  6600 , loss value =  2.098430755481211\n",
      "epochs =  0 , index =  6800 , loss value =  0.8699518045548221\n",
      "epochs =  0 , index =  7000 , loss value =  1.8965815170683584\n",
      "epochs =  0 , index =  7200 , loss value =  1.8407106229684325\n",
      "epochs =  0 , index =  7400 , loss value =  1.606542216744342\n",
      "epochs =  0 , index =  7600 , loss value =  1.4403596370635285\n",
      "epochs =  0 , index =  7800 , loss value =  1.2709433856806964\n",
      "epochs =  0 , index =  8000 , loss value =  0.6268332490312511\n",
      "epochs =  0 , index =  8200 , loss value =  5.2051066634334\n",
      "epochs =  0 , index =  8400 , loss value =  1.1900723634106516\n",
      "epochs =  0 , index =  8600 , loss value =  2.195464259424435\n",
      "epochs =  0 , index =  8800 , loss value =  0.8191996444952028\n",
      "epochs =  0 , index =  9000 , loss value =  1.2147036280666497\n",
      "epochs =  0 , index =  9200 , loss value =  0.7596370709275727\n",
      "epochs =  0 , index =  9400 , loss value =  0.851091936287177\n",
      "epochs =  0 , index =  9600 , loss value =  0.7164930007126707\n",
      "epochs =  0 , index =  9800 , loss value =  2.1219477098875466\n",
      "epochs =  0 , index =  10000 , loss value =  0.6474403769937016\n",
      "epochs =  0 , index =  10200 , loss value =  1.1288711596051735\n",
      "epochs =  0 , index =  10400 , loss value =  1.4760797580501597\n",
      "epochs =  0 , index =  10600 , loss value =  0.9089757330356886\n",
      "epochs =  0 , index =  10800 , loss value =  2.774042684845216\n",
      "epochs =  0 , index =  11000 , loss value =  0.6687572314951664\n",
      "epochs =  0 , index =  11200 , loss value =  0.6840909971744796\n",
      "epochs =  0 , index =  11400 , loss value =  0.7609672922293512\n",
      "epochs =  0 , index =  11600 , loss value =  4.576108589905207\n",
      "epochs =  0 , index =  11800 , loss value =  0.6540086033158276\n",
      "epochs =  0 , index =  12000 , loss value =  0.9709136026054038\n",
      "epochs =  0 , index =  12200 , loss value =  1.0415005183780375\n",
      "epochs =  0 , index =  12400 , loss value =  0.6759407088132219\n",
      "epochs =  0 , index =  12600 , loss value =  2.1000846392077372\n",
      "epochs =  0 , index =  12800 , loss value =  1.4779723192542729\n",
      "epochs =  0 , index =  13000 , loss value =  0.961572843390066\n",
      "epochs =  0 , index =  13200 , loss value =  1.6093856962713566\n",
      "epochs =  0 , index =  13400 , loss value =  1.1223637023916493\n",
      "epochs =  0 , index =  13600 , loss value =  0.6438983014917129\n",
      "epochs =  0 , index =  13800 , loss value =  1.1183138705907796\n",
      "epochs =  0 , index =  14000 , loss value =  0.6242976805519223\n",
      "epochs =  0 , index =  14200 , loss value =  0.826790881340853\n",
      "epochs =  0 , index =  14400 , loss value =  0.6874923313567413\n",
      "epochs =  0 , index =  14600 , loss value =  0.7604344533790558\n",
      "epochs =  0 , index =  14800 , loss value =  0.9237845904087175\n",
      "epochs =  0 , index =  15000 , loss value =  1.1468559139967933\n",
      "epochs =  0 , index =  15200 , loss value =  0.6031035876222224\n",
      "epochs =  0 , index =  15400 , loss value =  0.9023635252030561\n",
      "epochs =  0 , index =  15600 , loss value =  0.7011855577640175\n",
      "epochs =  0 , index =  15800 , loss value =  1.0717327726140737\n",
      "epochs =  0 , index =  16000 , loss value =  0.6130460340955307\n",
      "epochs =  0 , index =  16200 , loss value =  2.0830809022768064\n",
      "epochs =  0 , index =  16400 , loss value =  1.445712120597012\n",
      "epochs =  0 , index =  16600 , loss value =  1.8781682083203866\n",
      "epochs =  0 , index =  16800 , loss value =  0.6012609396955277\n",
      "epochs =  0 , index =  17000 , loss value =  0.9451653369483497\n",
      "epochs =  0 , index =  17200 , loss value =  1.1474675730521484\n",
      "epochs =  0 , index =  17400 , loss value =  2.4367224002052774\n",
      "epochs =  0 , index =  17600 , loss value =  0.9830370555927526\n",
      "epochs =  0 , index =  17800 , loss value =  0.83500797443433\n",
      "epochs =  0 , index =  18000 , loss value =  1.3026329228324354\n",
      "epochs =  0 , index =  18200 , loss value =  1.048646461251679\n",
      "epochs =  0 , index =  18400 , loss value =  0.7107095523639071\n",
      "epochs =  0 , index =  18600 , loss value =  0.9436162061019386\n",
      "epochs =  0 , index =  18800 , loss value =  0.5912673781233917\n",
      "epochs =  0 , index =  19000 , loss value =  0.8113352325381092\n",
      "epochs =  0 , index =  19200 , loss value =  0.7551271002841972\n",
      "epochs =  0 , index =  19400 , loss value =  0.673675422798433\n",
      "epochs =  0 , index =  19600 , loss value =  1.1956496677027812\n",
      "epochs =  0 , index =  19800 , loss value =  0.6740751587295866\n",
      "epochs =  0 , index =  20000 , loss value =  0.6949887576012783\n",
      "epochs =  0 , index =  20200 , loss value =  0.7436440608525412\n",
      "epochs =  0 , index =  20400 , loss value =  0.6226453213147454\n",
      "epochs =  0 , index =  20600 , loss value =  1.6436060606481275\n",
      "epochs =  0 , index =  20800 , loss value =  0.7027985619566073\n",
      "epochs =  0 , index =  21000 , loss value =  1.7057077933956748\n",
      "epochs =  0 , index =  21200 , loss value =  0.5917841650507657\n",
      "epochs =  0 , index =  21400 , loss value =  0.650337377802193\n",
      "epochs =  0 , index =  21600 , loss value =  0.6277144399605425\n",
      "epochs =  0 , index =  21800 , loss value =  0.6557141407437317\n",
      "epochs =  0 , index =  22000 , loss value =  1.1283607469410353\n",
      "epochs =  0 , index =  22200 , loss value =  4.302098898751232\n",
      "epochs =  0 , index =  22400 , loss value =  1.5907382749994616\n"
     ]
    }
   ],
   "source": [
    "# training data \n",
    "training_data = np.loadtxt(r'C:\\Users\\allma\\Desktop/mnist_train.csv', delimiter=',', dtype=np.float32)\n",
    "\n",
    "print(\"training_data.shape = \", training_data.shape)\n",
    "\n",
    "#hyper-parameter\n",
    "i_nodes = training_data.shape[1] - 1    # input nodes 개수\n",
    "h1_nodes = 30  # hidden nodes 개수. Test 8->30\n",
    "o_nodes = 10    # output nodes 개수\n",
    "lr = 1e-2      # learning rate\n",
    "epochs = 1   # 반복횟수\n",
    "\n",
    "# 손실함수 값을 저장할 list 생성\n",
    "loss_val_list = []\n",
    "\n",
    "# MNIST_Test 객체 생성\n",
    "obj = MNIST_Test(i_nodes, h1_nodes, o_nodes, lr)\n",
    "\n",
    "print(\"Neural Network Learning using Numerical Derivative...\")\n",
    "\n",
    "start_time = datetime.now()\n",
    "\n",
    "for step in range(epochs):\n",
    "    \n",
    "    for index in range(len(training_data)):    \n",
    "                \n",
    "        # input_data, target_data normalize : 입력데이터는 0~255 이기 때문에 가끔 overflow 발생, 따라서 모든 입력값을 0~1 값으로 normalize\n",
    "        input_data = ((training_data[index, 1:] / 255.0) * 0.99) + 0.01\n",
    "        \n",
    "        target_data = np.zeros(o_nodes) + 0.01    \n",
    "        target_data[int(training_data[index, 0])] = 0.99\n",
    "        \n",
    "        obj.train(input_data, target_data)\n",
    "        \n",
    "        if (index % 200 == 0):\n",
    "            print(\"epochs = \", step, \", index = \", index, \", loss value = \", obj.loss_val())\n",
    "            \n",
    "        # 손실함수 값 저장\n",
    "        loss_val_list.append(obj.loss_val())        \n",
    "\n",
    "end_time = datetime.now()\n",
    "        \n",
    "print(\"\")\n",
    "print(\"Elapsed Time => \", end_time - start_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
