{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 지도학습 logistic regression: classification\n",
    "# 출력값 y가 1또는 0만을 가져야하는 분류 시스템에서 함수 값으로 0~1사이의 값을 가지는 sigmoid 함수를 사용할 수 있음\n",
    "# 선형회귀 때와는 다른 손실함수가 필요함: cross-entropy... 유도하는건 수업 듣기만 하는걸로..\n",
    "# input -> linear regression -> classification sigmoid -> 손실함수의 최소값 -> 학습종료"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init error val =  21.473107477371187 init W =  [[0.4915231]] \n",
      " , b =  [0.05387912]\n",
      "step =  0 error_val =  7.220160074339277 W =  [[0.08697596]] , b =  [0.02164924]\n",
      "step =  400 error_val =  2.8530418360207026 W =  [[0.43142598]] , b =  [-4.28454772]\n",
      "step =  800 error_val =  1.756442730325061 W =  [[0.46035563]] , b =  [-5.73430465]\n",
      "step =  1200 error_val =  1.5021293339172426 W =  [[0.53603841]] , b =  [-6.74182713]\n",
      "step =  1600 error_val =  1.3417596455895704 W =  [[0.59635511]] , b =  [-7.54263362]\n",
      "step =  2000 error_val =  1.2280931990582815 W =  [[0.64725733]] , b =  [-8.2169945]\n",
      "step =  2400 error_val =  1.1416648169659838 W =  [[0.69172337]] , b =  [-8.80505806]\n",
      "step =  2800 error_val =  1.0727942466196192 W =  [[0.731473]] , b =  [-9.32998632]\n",
      "step =  3200 error_val =  1.0160477747806567 W =  [[0.76759646]] , b =  [-9.80644702]\n",
      "step =  3600 error_val =  0.9681050072686357 W =  [[0.80083204]] , b =  [-10.24436157]\n",
      "step =  4000 error_val =  0.9268049448350842 W =  [[0.83170487]] , b =  [-10.65077912]\n",
      "step =  4400 error_val =  0.8906709703791782 W =  [[0.860603]] , b =  [-11.03090278]\n",
      "step =  4800 error_val =  0.8586542069351841 W =  [[0.88782198]] , b =  [-11.38869178]\n",
      "step =  5200 error_val =  0.8299856675620484 W =  [[0.91359265]] , b =  [-11.7272348]\n",
      "step =  5600 error_val =  0.804086543264275 W =  [[0.93809899]] , b =  [-12.04899181]\n",
      "step =  6000 error_val =  0.7805113834414206 W =  [[0.96149027]] , b =  [-12.35595672]\n",
      "step =  6400 error_val =  0.7589107884381694 W =  [[0.98388943]] , b =  [-12.64977017]\n",
      "step =  6800 error_val =  0.7390061491087282 W =  [[1.00539901]] , b =  [-12.93179978]\n",
      "step =  7200 error_val =  0.7205720834452229 W =  [[1.02610553]] , b =  [-13.20319869]\n",
      "step =  7600 error_val =  0.7034239390494822 W =  [[1.04608275]] , b =  [-13.464949]\n",
      "step =  8000 error_val =  0.6874087173524215 W =  [[1.06539408]] , b =  [-13.71789469]\n",
      "step =  8400 error_val =  0.6723983626064032 W =  [[1.08409448]] , b =  [-13.96276683]\n",
      "step =  8800 error_val =  0.6582847187950006 W =  [[1.10223194]] , b =  [-14.20020331]\n",
      "step =  9200 error_val =  0.6449756845939884 W =  [[1.11984863]] , b =  [-14.43076434]\n",
      "step =  9600 error_val =  0.63239224312474 W =  [[1.13698182]] , b =  [-14.65494484]\n",
      "step =  10000 error_val =  0.6204661400242445 W =  [[1.15366463]] , b =  [-14.87318444]\n"
     ]
    }
   ],
   "source": [
    "# 예제\n",
    "import numpy as np\n",
    "\n",
    "x_data = np.array([2,4,6,8,10,12,14,16,18,20]).reshape(10,1)\n",
    "t_data = np.array([0,0,0,0,0,0,1,1,1,1]).reshape(10,1)\n",
    "\n",
    "W = np.random.rand(1,1)\n",
    "b = np.random.rand(1)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def loss_func(x, t):\n",
    "    delta = 1e-7 # log 내 0이 들어가면 무한대로 갈 수 있기 때문에 이를 방지하기 위함\n",
    "    z = np.dot(x, W) + b\n",
    "    y = sigmoid(z)\n",
    "    # cross-entropy\n",
    "    return -np.sum(t*np.log(y+delta) + (1-t)*np.log((1-y)+delta))\n",
    "\n",
    "def numerical_derivative(f, x):\n",
    "    delta_x = 1e-4\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    \n",
    "    while not it.finished:\n",
    "        idx = it.multi_index\n",
    "        tmp_val = x[idx]\n",
    "        \n",
    "        x[idx] = float(tmp_val) + delta_x\n",
    "        fx1 = f(x)\n",
    "        \n",
    "        x[idx] = tmp_val - delta_x\n",
    "        fx2 = f(x)\n",
    "        \n",
    "        grad[idx] = (fx1-fx2) / (2*delta_x)\n",
    "        \n",
    "        x[idx] = tmp_val\n",
    "        it. iternext()\n",
    "    \n",
    "    return grad\n",
    "\n",
    "def error_val(x, t):\n",
    "    delta = 1e-7 # log 내 0이 들어가면 무한대로 갈 수 있기 때문에 이를 방지하기 위함\n",
    "    z = np.dot(x, W) + b\n",
    "    y = sigmoid(z)\n",
    "    # cross-entropy\n",
    "    return -np.sum(t*np.log(y+delta) + (1-t)*np.log((1-y)+delta))\n",
    "\n",
    "def predict(x):\n",
    "    z = np.dot(x, W) + b\n",
    "    y = sigmoid(z)\n",
    "    \n",
    "    if y > 0.5:\n",
    "        result = 1 # True\n",
    "    else:\n",
    "        result = 0 # False\n",
    "        \n",
    "    return y, result\n",
    "\n",
    "learning_rate = 1e-2 # 발산하는 경우 바꾸어서 실행\n",
    "f = lambda x : loss_func(x_data, t_data) # f(x) = loss_func(x_data, t_data)\n",
    "print(\"init error val = \", error_val(x_data, t_data), \"init W = \", W, \"\\n\", \", b = \", b)\n",
    "\n",
    "for step in range(10001):\n",
    "    W -= learning_rate * numerical_derivative(f, W)\n",
    "    b -= learning_rate * numerical_derivative(f, b)\n",
    "    \n",
    "    if (step%400 == 0):\n",
    "        print(\"step = \", step, \"error_val = \", error_val(x_data, t_data), \"W = \", W, \", b = \", b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[0.99913026]]), 1)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[1.10597119e-05]]), 0)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init error val =  5.5980792072178325 init W =  [[0.04088418]\n",
      " [0.03597004]] \n",
      " , b =  [0.07927207]\n",
      "step =  0 error_val =  5.011208046533293 W =  [[ 0.10876297]\n",
      " [-0.01841746]] , b =  [0.06419685]\n",
      "step =  400 error_val =  2.1679435414442048 W =  [[ 0.43409044]\n",
      " [-0.07854901]] , b =  [-2.83074072]\n",
      "step =  800 error_val =  1.5493334624782291 W =  [[ 0.54618189]\n",
      " [-0.02204632]] , b =  [-4.39310614]\n",
      "step =  1200 error_val =  1.2572043701495546 W =  [[0.63120281]\n",
      " [0.01191983]] , b =  [-5.47048464]\n",
      "step =  1600 error_val =  1.0836212543771864 W =  [[0.70020185]\n",
      " [0.03645293]] , b =  [-6.30176123]\n",
      "step =  2000 error_val =  0.9663805184100692 W =  [[0.75855703]\n",
      " [0.05615796]] , b =  [-6.98512055]\n",
      "step =  2400 error_val =  0.8804882270252694 W =  [[0.80927068]\n",
      " [0.07311732]] , b =  [-7.57006083]\n",
      "step =  2800 error_val =  0.8139542862262361 W =  [[0.8541923 ]\n",
      " [0.08842038]] , b =  [-8.08487386]\n",
      "step =  3200 error_val =  0.7602962493342065 W =  [[0.89454601]\n",
      " [0.1026922 ]] , b =  [-8.54717983]\n",
      "step =  3600 error_val =  0.715692163106299 W =  [[0.93118778]\n",
      " [0.11631366]] , b =  [-8.96866485]\n",
      "step =  4000 error_val =  0.677735351786339 W =  [[0.96474299]\n",
      " [0.12952523]] , b =  [-9.35746217]\n",
      "step =  4400 error_val =  0.6448299280479745 W =  [[0.99568555]\n",
      " [0.14248089]] , b =  [-9.71945252]\n",
      "step =  4800 error_val =  0.6158726569707325 W =  [[1.02438589]\n",
      " [0.15527814]] , b =  [-10.0590213]\n",
      "step =  5200 error_val =  0.5900743447152765 W =  [[1.05114141]\n",
      " [0.16797609]] , b =  [-10.37952315]\n",
      "step =  5600 error_val =  0.5668540983338735 W =  [[1.07619631]\n",
      " [0.18060695]] , b =  [-10.68357948]\n",
      "step =  6000 error_val =  0.5457738844159005 W =  [[1.09975492]\n",
      " [0.19318396]] , b =  [-10.97327604]\n",
      "step =  6400 error_val =  0.5264964699730709 W =  [[1.12199077]\n",
      " [0.20570709]] , b =  [-11.25029834]\n",
      "step =  6800 error_val =  0.5087574983643408 W =  [[1.14305284]\n",
      " [0.21816738]] , b =  [-11.51602698]\n",
      "step =  7200 error_val =  0.4923464196325444 W =  [[1.16307007]\n",
      " [0.23055027]] , b =  [-11.77160644]\n",
      "step =  7600 error_val =  0.4770931434733887 W =  [[1.18215456]\n",
      " [0.24283824]] , b =  [-12.01799573]\n",
      "step =  8000 error_val =  0.46285849470426377 W =  [[1.20040406]\n",
      " [0.25501272]] , b =  [-12.25600637]\n",
      "step =  8400 error_val =  0.449527258556572 W =  [[1.21790385]\n",
      " [0.26705562]] , b =  [-12.48633134]\n",
      "step =  8800 error_val =  0.43700302911220124 W =  [[1.23472835]\n",
      " [0.27895023]] , b =  [-12.70956742]\n",
      "step =  9200 error_val =  0.4252043379260596 W =  [[1.25094246]\n",
      " [0.29068193]] , b =  [-12.92623271]\n",
      "step =  9600 error_val =  0.4140617073402022 W =  [[1.26660277]\n",
      " [0.30223843]] , b =  [-13.13678033]\n",
      "step =  10000 error_val =  0.40351538187906977 W =  [[1.2817586 ]\n",
      " [0.31360992]] , b =  [-13.34160948]\n",
      "step =  10400 error_val =  0.3935135635055164 W =  [[1.29645294]\n",
      " [0.32478896]] , b =  [-13.54107416]\n",
      "step =  10800 error_val =  0.3840110256557941 W =  [[1.3107233 ]\n",
      " [0.33577036]] , b =  [-13.73549032]\n",
      "step =  11200 error_val =  0.37496801497075993 W =  [[1.32460248]\n",
      " [0.34655091]] , b =  [-13.92514161]\n",
      "step =  11600 error_val =  0.3663493735793431 W =  [[1.33811919]\n",
      " [0.35712913]] , b =  [-14.11028407]\n",
      "step =  12000 error_val =  0.3581238319033718 W =  [[1.35129865]\n",
      " [0.36750501]] , b =  [-14.29115003]\n",
      "step =  12400 error_val =  0.35026343435291063 W =  [[1.36416308]\n",
      " [0.37767975]] , b =  [-14.46795132]\n",
      "step =  12800 error_val =  0.3427430693691796 W =  [[1.37673216]\n",
      " [0.38765556]] , b =  [-14.64088186]\n",
      "step =  13200 error_val =  0.33554008200104396 W =  [[1.38902336]\n",
      " [0.39743543]] , b =  [-14.81011998]\n",
      "step =  13600 error_val =  0.328633952223243 W =  [[1.40105225]\n",
      " [0.40702296]] , b =  [-14.97583021]\n",
      "step =  14000 error_val =  0.3220060259857879 W =  [[1.41283283]\n",
      " [0.41642219]] , b =  [-15.13816487]\n",
      "step =  14400 error_val =  0.3156392888460147 W =  [[1.42437767]\n",
      " [0.42563752]] , b =  [-15.29726547]\n",
      "step =  14800 error_val =  0.30951817421930533 W =  [[1.43569815]\n",
      " [0.43467357]] , b =  [-15.45326377]\n",
      "step =  15200 error_val =  0.30362839995805374 W =  [[1.44680464]\n",
      " [0.44353509]] , b =  [-15.60628283]\n",
      "step =  15600 error_val =  0.29795682826107783 W =  [[1.45770657]\n",
      " [0.45222691]] , b =  [-15.75643785]\n",
      "step =  16000 error_val =  0.292491344917406 W =  [[1.4684126 ]\n",
      " [0.46075389]] , b =  [-15.90383689]\n",
      "step =  16400 error_val =  0.28722075467103525 W =  [[1.47893073]\n",
      " [0.46912085]] , b =  [-16.04858152]\n",
      "step =  16800 error_val =  0.2821346901061745 W =  [[1.4892683 ]\n",
      " [0.47733257]] , b =  [-16.19076742]\n",
      "step =  17200 error_val =  0.27722353193681704 W =  [[1.49943217]\n",
      " [0.48539373]] , b =  [-16.33048483]\n",
      "step =  17600 error_val =  0.2724783389683783 W =  [[1.50942868]\n",
      " [0.49330892]] , b =  [-16.46781903]\n",
      "step =  18000 error_val =  0.2678907863050734 W =  [[1.51926379]\n",
      " [0.5010826 ]] , b =  [-16.60285072]\n",
      "step =  18400 error_val =  0.26345311062254295 W =  [[1.52894304]\n",
      " [0.50871911]] , b =  [-16.73565637]\n",
      "step =  18800 error_val =  0.25915806152310394 W =  [[1.53847167]\n",
      " [0.51622267]] , b =  [-16.86630855]\n",
      "step =  19200 error_val =  0.2549988581512988 W =  [[1.54785457]\n",
      " [0.52359734]] , b =  [-16.99487617]\n",
      "step =  19600 error_val =  0.2509691503785524 W =  [[1.55709639]\n",
      " [0.53084706]] , b =  [-17.12142481]\n",
      "step =  20000 error_val =  0.24706298397237628 W =  [[1.56620152]\n",
      " [0.53797563]] , b =  [-17.24601686]\n",
      "step =  20400 error_val =  0.2432747692541184 W =  [[1.57517409]\n",
      " [0.5449867 ]] , b =  [-17.36871182]\n",
      "step =  20800 error_val =  0.2395992528221502 W =  [[1.58401806]\n",
      " [0.55188379]] , b =  [-17.48956643]\n",
      "step =  21200 error_val =  0.23603149197769757 W =  [[1.59273715]\n",
      " [0.55867032]] , b =  [-17.60863485]\n",
      "step =  21600 error_val =  0.23256683154163377 W =  [[1.60133495]\n",
      " [0.56534953]] , b =  [-17.72596887]\n",
      "step =  22000 error_val =  0.2292008827930127 W =  [[1.60981483]\n",
      " [0.57192457]] , b =  [-17.841618]\n",
      "step =  22400 error_val =  0.2259295042953977 W =  [[1.61818006]\n",
      " [0.57839846]] , b =  [-17.95562962]\n",
      "step =  22800 error_val =  0.22274878440823437 W =  [[1.62643373]\n",
      " [0.58477411]] , b =  [-18.06804912]\n",
      "step =  23200 error_val =  0.21965502530559472 W =  [[1.6345788 ]\n",
      " [0.59105431]] , b =  [-18.17892001]\n",
      "step =  23600 error_val =  0.21664472834703366 W =  [[1.64261811]\n",
      " [0.59724175]] , b =  [-18.28828401]\n",
      "step =  24000 error_val =  0.213714580663905 W =  [[1.6505544 ]\n",
      " [0.60333902]] , b =  [-18.39618117]\n",
      "step =  24400 error_val =  0.21086144284089484 W =  [[1.65839027]\n",
      " [0.6093486 ]] , b =  [-18.50264996]\n",
      "step =  24800 error_val =  0.20808233758625816 W =  [[1.66612823]\n",
      " [0.61527288]] , b =  [-18.60772733]\n",
      "step =  25200 error_val =  0.20537443929677063 W =  [[1.6737707 ]\n",
      " [0.62111416]] , b =  [-18.71144881]\n",
      "step =  25600 error_val =  0.20273506443332098 W =  [[1.68131999]\n",
      " [0.62687466]] , b =  [-18.81384858]\n",
      "step =  26000 error_val =  0.2001616626328772 W =  [[1.68877835]\n",
      " [0.63255651]] , b =  [-18.91495955]\n",
      "step =  26400 error_val =  0.19765180849003636 W =  [[1.69614793]\n",
      " [0.63816177]] , b =  [-19.0148134]\n",
      "step =  26800 error_val =  0.19520319394875726 W =  [[1.70343079]\n",
      " [0.64369241]] , b =  [-19.11344066]\n",
      "step =  27200 error_val =  0.1928136212507981 W =  [[1.71062894]\n",
      " [0.64915034]] , b =  [-19.21087076]\n",
      "step =  27600 error_val =  0.19048099639288427 W =  [[1.71774431]\n",
      " [0.65453739]] , b =  [-19.30713208]\n",
      "step =  28000 error_val =  0.1882033230495404 W =  [[1.72477877]\n",
      " [0.65985533]] , b =  [-19.402252]\n",
      "step =  28400 error_val =  0.18597869692266392 W =  [[1.73173412]\n",
      " [0.66510588]] , b =  [-19.49625697]\n",
      "step =  28800 error_val =  0.18380530048244578 W =  [[1.73861209]\n",
      " [0.67029067]] , b =  [-19.58917249]\n",
      "step =  29200 error_val =  0.18168139806828812 W =  [[1.74541437]\n",
      " [0.6754113 ]] , b =  [-19.68102325]\n",
      "step =  29600 error_val =  0.17960533132042136 W =  [[1.75214258]\n",
      " [0.6804693 ]] , b =  [-19.77183306]\n",
      "step =  30000 error_val =  0.1775755149164721 W =  [[1.75879831]\n",
      " [0.68546616]] , b =  [-19.86162498]\n",
      "step =  30400 error_val =  0.17559043258888407 W =  [[1.76538308]\n",
      " [0.6904033 ]] , b =  [-19.9504213]\n",
      "step =  30800 error_val =  0.1736486334018076 W =  [[1.77189836]\n",
      " [0.6952821 ]] , b =  [-20.03824359]\n",
      "step =  31200 error_val =  0.17174872826761778 W =  [[1.77834558]\n",
      " [0.7001039 ]] , b =  [-20.12511272]\n",
      "step =  31600 error_val =  0.1698893866850296 W =  [[1.78472614]\n",
      " [0.70487   ]] , b =  [-20.21104891]\n",
      "step =  32000 error_val =  0.1680693336825142 W =  [[1.79104139]\n",
      " [0.70958164]] , b =  [-20.29607174]\n",
      "step =  32400 error_val =  0.1662873469518035 W =  [[1.79729261]\n",
      " [0.71424003]] , b =  [-20.38020019]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step =  32800 error_val =  0.16454225415784365 W =  [[1.80348109]\n",
      " [0.71884634]] , b =  [-20.46345264]\n",
      "step =  33200 error_val =  0.1628329304124679 W =  [[1.80960804]\n",
      " [0.7234017 ]] , b =  [-20.54584694]\n",
      "step =  33600 error_val =  0.16115829590027228 W =  [[1.81567468]\n",
      " [0.72790721]] , b =  [-20.62740038]\n",
      "step =  34000 error_val =  0.15951731364590033 W =  [[1.82168214]\n",
      " [0.73236394]] , b =  [-20.70812974]\n",
      "step =  34400 error_val =  0.15790898741315515 W =  [[1.82763156]\n",
      " [0.7367729 ]] , b =  [-20.7880513]\n",
      "step =  34800 error_val =  0.15633235972669657 W =  [[1.83352404]\n",
      " [0.74113509]] , b =  [-20.86718088]\n",
      "step =  35200 error_val =  0.1547865100081014 W =  [[1.83936063]\n",
      " [0.7454515 ]] , b =  [-20.94553384]\n",
      "step =  35600 error_val =  0.15327055281864257 W =  [[1.84514237]\n",
      " [0.74972304]] , b =  [-21.02312507]\n",
      "step =  36000 error_val =  0.15178363620158566 W =  [[1.85087027]\n",
      " [0.75395063]] , b =  [-21.09996909]\n",
      "step =  36400 error_val =  0.15032494011759673 W =  [[1.85654531]\n",
      " [0.75813515]] , b =  [-21.17607996]\n",
      "step =  36800 error_val =  0.14889367496709882 W =  [[1.86216844]\n",
      " [0.76227746]] , b =  [-21.25147139]\n",
      "step =  37200 error_val =  0.14748908019398452 W =  [[1.86774058]\n",
      " [0.7663784 ]] , b =  [-21.32615668]\n",
      "step =  37600 error_val =  0.14611042296554044 W =  [[1.87326264]\n",
      " [0.77043876]] , b =  [-21.40014879]\n",
      "step =  38000 error_val =  0.14475699692365923 W =  [[1.8787355 ]\n",
      " [0.77445933]] , b =  [-21.47346031]\n",
      "step =  38400 error_val =  0.14342812100307117 W =  [[1.88416001]\n",
      " [0.77844088]] , b =  [-21.5461035]\n",
      "step =  38800 error_val =  0.14212313831214454 W =  [[1.88953702]\n",
      " [0.78238414]] , b =  [-21.6180903]\n",
      "step =  39200 error_val =  0.14084141507268505 W =  [[1.89486732]\n",
      " [0.78628983]] , b =  [-21.68943234]\n",
      "step =  39600 error_val =  0.1395823396149692 W =  [[1.90015172]\n",
      " [0.79015866]] , b =  [-21.76014091]\n",
      "step =  40000 error_val =  0.1383453214246991 W =  [[1.90539098]\n",
      " [0.79399131]] , b =  [-21.83022705]\n",
      "step =  40400 error_val =  0.13712979023888075 W =  [[1.91058587]\n",
      " [0.79778844]] , b =  [-21.89970149]\n",
      "step =  40800 error_val =  0.13593519518755065 W =  [[1.91573711]\n",
      " [0.80155068]] , b =  [-21.96857471]\n",
      "step =  41200 error_val =  0.1347610039787543 W =  [[1.92084542]\n",
      " [0.80527868]] , b =  [-22.03685689]\n",
      "step =  41600 error_val =  0.1336067021242938 W =  [[1.9259115 ]\n",
      " [0.80897304]] , b =  [-22.10455799]\n",
      "step =  42000 error_val =  0.13247179220375194 W =  [[1.93093604]\n",
      " [0.81263435]] , b =  [-22.17168769]\n",
      "step =  42400 error_val =  0.13135579316475793 W =  [[1.9359197]\n",
      " [0.8162632]] , b =  [-22.23825547]\n",
      "step =  42800 error_val =  0.13025823965728195 W =  [[1.94086313]\n",
      " [0.81986014]] , b =  [-22.30427054]\n",
      "step =  43200 error_val =  0.12917868140012642 W =  [[1.94576696]\n",
      " [0.82342573]] , b =  [-22.36974191]\n",
      "step =  43600 error_val =  0.12811668257785983 W =  [[1.95063183]\n",
      " [0.82696051]] , b =  [-22.43467836]\n",
      "step =  44000 error_val =  0.12707182126631572 W =  [[1.95545833]\n",
      " [0.83046499]] , b =  [-22.49908848]\n",
      "step =  44400 error_val =  0.12604368888532666 W =  [[1.96024705]\n",
      " [0.83393969]] , b =  [-22.56298062]\n",
      "step =  44800 error_val =  0.12503188967700177 W =  [[1.96499858]\n",
      " [0.8373851 ]] , b =  [-22.62636298]\n",
      "step =  45200 error_val =  0.12403604020830208 W =  [[1.96971348]\n",
      " [0.84080171]] , b =  [-22.68924353]\n",
      "step =  45600 error_val =  0.12305576889652547 W =  [[1.9743923 ]\n",
      " [0.84418998]] , b =  [-22.75163007]\n",
      "step =  46000 error_val =  0.12209071555650436 W =  [[1.97903559]\n",
      " [0.84755039]] , b =  [-22.81353022]\n",
      "step =  46400 error_val =  0.12114053096839314 W =  [[1.98364387]\n",
      " [0.85088337]] , b =  [-22.87495144]\n",
      "step =  46800 error_val =  0.1202048764648928 W =  [[1.98821766]\n",
      " [0.85418937]] , b =  [-22.93590099]\n",
      "step =  47200 error_val =  0.11928342353699295 W =  [[1.99275747]\n",
      " [0.85746883]] , b =  [-22.996386]\n",
      "step =  47600 error_val =  0.11837585345719545 W =  [[1.99726379]\n",
      " [0.86072214]] , b =  [-23.05641342]\n",
      "step =  48000 error_val =  0.11748185691933935 W =  [[2.0017371 ]\n",
      " [0.86394974]] , b =  [-23.11599004]\n",
      "step =  48400 error_val =  0.11660113369422123 W =  [[2.00617789]\n",
      " [0.86715201]] , b =  [-23.17512254]\n",
      "step =  48800 error_val =  0.1157333923001829 W =  [[2.0105866 ]\n",
      " [0.87032934]] , b =  [-23.2338174]\n",
      "step =  49200 error_val =  0.11487834968784962 W =  [[2.01496371]\n",
      " [0.87348212]] , b =  [-23.29208099]\n",
      "step =  49600 error_val =  0.11403573093845601 W =  [[2.01930964]\n",
      " [0.87661073]] , b =  [-23.34991956]\n",
      "step =  50000 error_val =  0.11320526897495099 W =  [[2.02362485]\n",
      " [0.87971551]] , b =  [-23.40733918]\n",
      "step =  50400 error_val =  0.11238670428530906 W =  [[2.02790974]\n",
      " [0.88279684]] , b =  [-23.46434583]\n",
      "step =  50800 error_val =  0.11157978465746762 W =  [[2.03216475]\n",
      " [0.88585506]] , b =  [-23.52094534]\n",
      "step =  51200 error_val =  0.11078426492531072 W =  [[2.03639028]\n",
      " [0.8888905 ]] , b =  [-23.57714344]\n",
      "step =  51600 error_val =  0.10999990672512806 W =  [[2.04058673]\n",
      " [0.89190351]] , b =  [-23.63294573]\n",
      "step =  52000 error_val =  0.10922647826212752 W =  [[2.0447545 ]\n",
      " [0.89489442]] , b =  [-23.68835768]\n",
      "step =  52400 error_val =  0.1084637540864303 W =  [[2.04889396]\n",
      " [0.89786352]] , b =  [-23.74338467]\n",
      "step =  52800 error_val =  0.10771151487818677 W =  [[2.05300551]\n",
      " [0.90081115]] , b =  [-23.79803197]\n",
      "step =  53200 error_val =  0.1069695472413125 W =  [[2.05708949]\n",
      " [0.90373761]] , b =  [-23.85230472]\n",
      "step =  53600 error_val =  0.10623764350552707 W =  [[2.06114629]\n",
      " [0.90664319]] , b =  [-23.90620798]\n",
      "step =  54000 error_val =  0.10551560153621123 W =  [[2.06517626]\n",
      " [0.9095282 ]] , b =  [-23.9597467]\n",
      "step =  54400 error_val =  0.10480322455178863 W =  [[2.06917973]\n",
      " [0.9123929 ]] , b =  [-24.01292573]\n",
      "step =  54800 error_val =  0.10410032094829205 W =  [[2.07315707]\n",
      " [0.9152376 ]] , b =  [-24.06574983]\n",
      "step =  55200 error_val =  0.10340670413071318 W =  [[2.07710859]\n",
      " [0.91806255]] , b =  [-24.11822366]\n",
      "step =  55600 error_val =  0.1027221923509594 W =  [[2.08103463]\n",
      " [0.92086804]] , b =  [-24.1703518]\n",
      "step =  56000 error_val =  0.10204660855201106 W =  [[2.08493552]\n",
      " [0.92365433]] , b =  [-24.22213873]\n",
      "step =  56400 error_val =  0.10137978021800999 W =  [[2.08881157]\n",
      " [0.92642167]] , b =  [-24.27358884]\n",
      "step =  56800 error_val =  0.10072153923010084 W =  [[2.09266309]\n",
      " [0.92917031]] , b =  [-24.32470646]\n",
      "step =  57200 error_val =  0.10007172172767267 W =  [[2.09649039]\n",
      " [0.93190052]] , b =  [-24.37549581]\n",
      "step =  57600 error_val =  0.09943016797483371 W =  [[2.10029376]\n",
      " [0.93461252]] , b =  [-24.42596105]\n",
      "step =  58000 error_val =  0.098796722231848 W =  [[2.10407351]\n",
      " [0.93730657]] , b =  [-24.47610625]\n",
      "step =  58400 error_val =  0.09817123263136274 W =  [[2.10782991]\n",
      " [0.93998288]] , b =  [-24.52593541]\n",
      "step =  58800 error_val =  0.09755355105918939 W =  [[2.11156326]\n",
      " [0.94264171]] , b =  [-24.57545246]\n",
      "step =  59200 error_val =  0.09694353303942434 W =  [[2.11527382]\n",
      " [0.94528326]] , b =  [-24.62466124]\n",
      "step =  59600 error_val =  0.09634103762380249 W =  [[2.11896189]\n",
      " [0.94790776]] , b =  [-24.67356554]\n",
      "step =  60000 error_val =  0.09574592728500365 W =  [[2.12262771]\n",
      " [0.95051543]] , b =  [-24.72216908]\n",
      "step =  60400 error_val =  0.09515806781384044 W =  [[2.12627156]\n",
      " [0.95310648]] , b =  [-24.77047549]\n",
      "step =  60800 error_val =  0.09457732822009063 W =  [[2.1298937 ]\n",
      " [0.95568111]] , b =  [-24.81848837]\n",
      "step =  61200 error_val =  0.09400358063688549 W =  [[2.13349439]\n",
      " [0.95823954]] , b =  [-24.86621122]\n",
      "step =  61600 error_val =  0.09343670022843813 W =  [[2.13707386]\n",
      " [0.96078197]] , b =  [-24.9136475]\n",
      "step =  62000 error_val =  0.09287656510107634 W =  [[2.14063237]\n",
      " [0.96330859]] , b =  [-24.96080061]\n",
      "step =  62400 error_val =  0.09232305621731553 W =  [[2.14417016]\n",
      " [0.96581959]] , b =  [-25.00767387]\n",
      "step =  62800 error_val =  0.0917760573129663 W =  [[2.14768746]\n",
      " [0.96831517]] , b =  [-25.05427056]\n",
      "step =  63200 error_val =  0.09123545481709137 W =  [[2.15118452]\n",
      " [0.97079551]] , b =  [-25.10059391]\n",
      "step =  63600 error_val =  0.09070113777469323 W =  [[2.15466156]\n",
      " [0.9732608 ]] , b =  [-25.14664707]\n",
      "step =  64000 error_val =  0.09017299777207631 W =  [[2.1581188 ]\n",
      " [0.97571121]] , b =  [-25.19243315]\n",
      "step =  64400 error_val =  0.08965092886471326 W =  [[2.16155648]\n",
      " [0.97814693]] , b =  [-25.23795521]\n",
      "step =  64800 error_val =  0.08913482750759602 W =  [[2.16497479]\n",
      " [0.98056812]] , b =  [-25.28321625]\n",
      "step =  65200 error_val =  0.08862459248784876 W =  [[2.16837397]\n",
      " [0.98297497]] , b =  [-25.32821922]\n",
      "step =  65600 error_val =  0.08812012485966175 W =  [[2.17175422]\n",
      " [0.98536763]] , b =  [-25.37296704]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step =  66000 error_val =  0.08762132788135828 W =  [[2.17511576]\n",
      " [0.98774627]] , b =  [-25.41746254]\n",
      "step =  66400 error_val =  0.087128106954542 W =  [[2.17845877]\n",
      " [0.99011105]] , b =  [-25.46170855]\n",
      "step =  66800 error_val =  0.08664036956525699 W =  [[2.18178348]\n",
      " [0.99246214]] , b =  [-25.50570782]\n",
      "step =  67200 error_val =  0.0861580252270472 W =  [[2.18509006]\n",
      " [0.99479968]] , b =  [-25.54946307]\n",
      "step =  67600 error_val =  0.085680985425904 W =  [[2.18837872]\n",
      " [0.99712384]] , b =  [-25.59297696]\n",
      "step =  68000 error_val =  0.08520916356694597 W =  [[2.19164966]\n",
      " [0.99943476]] , b =  [-25.63625214]\n",
      "step =  68400 error_val =  0.0847424749228651 W =  [[2.19490305]\n",
      " [1.0017326 ]] , b =  [-25.67929118]\n",
      "step =  68800 error_val =  0.08428083658397363 W =  [[2.19813908]\n",
      " [1.00401749]] , b =  [-25.72209662]\n",
      "step =  69200 error_val =  0.08382416740988442 W =  [[2.20135794]\n",
      " [1.00628959]] , b =  [-25.76467098]\n",
      "step =  69600 error_val =  0.08337238798270823 W =  [[2.20455981]\n",
      " [1.00854903]] , b =  [-25.80701671]\n",
      "step =  70000 error_val =  0.08292542056170023 W =  [[2.20774486]\n",
      " [1.01079595]] , b =  [-25.84913623]\n",
      "step =  70400 error_val =  0.08248318903936577 W =  [[2.21091327]\n",
      " [1.0130305 ]] , b =  [-25.89103195]\n",
      "step =  70800 error_val =  0.08204561889889904 W =  [[2.21406521]\n",
      " [1.0152528 ]] , b =  [-25.9327062]\n",
      "step =  71200 error_val =  0.08161263717299327 W =  [[2.21720084]\n",
      " [1.01746299]] , b =  [-25.97416129]\n",
      "step =  71600 error_val =  0.08118417240385259 W =  [[2.22032034]\n",
      " [1.01966119]] , b =  [-26.01539951]\n",
      "step =  72000 error_val =  0.08076015460448412 W =  [[2.22342387]\n",
      " [1.02184755]] , b =  [-26.05642309]\n",
      "step =  72400 error_val =  0.08034051522116019 W =  [[2.22651159]\n",
      " [1.02402217]] , b =  [-26.09723425]\n",
      "step =  72800 error_val =  0.07992518709702198 W =  [[2.22958365]\n",
      " [1.0261852 ]] , b =  [-26.13783515]\n",
      "step =  73200 error_val =  0.0795141044367767 W =  [[2.23264022]\n",
      " [1.02833674]] , b =  [-26.17822793]\n",
      "step =  73600 error_val =  0.07910720277245858 W =  [[2.23568145]\n",
      " [1.03047692]] , b =  [-26.21841471]\n",
      "step =  74000 error_val =  0.07870441893021943 W =  [[2.2387075 ]\n",
      " [1.03260586]] , b =  [-26.25839756]\n",
      "step =  74400 error_val =  0.07830569099812604 W =  [[2.2417185 ]\n",
      " [1.03472367]] , b =  [-26.29817852]\n",
      "step =  74800 error_val =  0.0779109582948731 W =  [[2.24471461]\n",
      " [1.03683048]] , b =  [-26.33775962]\n",
      "step =  75200 error_val =  0.07752016133944706 W =  [[2.24769598]\n",
      " [1.03892639]] , b =  [-26.37714282]\n",
      "step =  75600 error_val =  0.07713324182169105 W =  [[2.25066274]\n",
      " [1.04101151]] , b =  [-26.4163301]\n",
      "step =  76000 error_val =  0.07675014257369675 W =  [[2.25361505]\n",
      " [1.04308595]] , b =  [-26.45532338]\n",
      "step =  76400 error_val =  0.07637080754205317 W =  [[2.25655303]\n",
      " [1.04514983]] , b =  [-26.49412455]\n",
      "step =  76800 error_val =  0.0759951817608938 W =  [[2.25947683]\n",
      " [1.04720324]] , b =  [-26.53273548]\n",
      "step =  77200 error_val =  0.07562321132572677 W =  [[2.26238658]\n",
      " [1.0492463 ]] , b =  [-26.57115803]\n",
      "step =  77600 error_val =  0.07525484336800138 W =  [[2.26528241]\n",
      " [1.0512791 ]] , b =  [-26.60939401]\n",
      "step =  78000 error_val =  0.07489002603039872 W =  [[2.26816446]\n",
      " [1.05330176]] , b =  [-26.6474452]\n",
      "step =  78400 error_val =  0.07452870844285911 W =  [[2.27103285]\n",
      " [1.05531436]] , b =  [-26.68531338]\n",
      "step =  78800 error_val =  0.0741708406992319 W =  [[2.27388772]\n",
      " [1.057317  ]] , b =  [-26.72300029]\n",
      "step =  79200 error_val =  0.07381637383463698 W =  [[2.27672918]\n",
      " [1.0593098 ]] , b =  [-26.76050764]\n",
      "step =  79600 error_val =  0.07346525980341591 W =  [[2.27955737]\n",
      " [1.06129283]] , b =  [-26.79783712]\n",
      "step =  80000 error_val =  0.07311745145773821 W =  [[2.28237241]\n",
      " [1.0632662 ]] , b =  [-26.83499041]\n",
      "step =  80400 error_val =  0.0727729025267668 W =  [[2.28517441]\n",
      " [1.06522999]] , b =  [-26.87196914]\n",
      "step =  80800 error_val =  0.07243156759642558 W =  [[2.2879635]\n",
      " [1.0671843]] , b =  [-26.90877495]\n",
      "step =  81200 error_val =  0.07209340208969758 W =  [[2.2907398 ]\n",
      " [1.06912923]] , b =  [-26.94540942]\n",
      "step =  81600 error_val =  0.07175836224748733 W =  [[2.29350341]\n",
      " [1.07106485]] , b =  [-26.98187414]\n",
      "step =  82000 error_val =  0.07142640511000307 W =  [[2.29625446]\n",
      " [1.07299126]] , b =  [-27.01817067]\n",
      "step =  82400 error_val =  0.07109748849862119 W =  [[2.29899305]\n",
      " [1.07490855]] , b =  [-27.05430053]\n",
      "step =  82800 error_val =  0.07077157099826355 W =  [[2.30171931]\n",
      " [1.07681679]] , b =  [-27.09026525]\n",
      "step =  83200 error_val =  0.07044861194023974 W =  [[2.30443333]\n",
      " [1.07871608]] , b =  [-27.12606631]\n",
      "step =  83600 error_val =  0.07012857138555323 W =  [[2.30713524]\n",
      " [1.0806065 ]] , b =  [-27.1617052]\n",
      "step =  84000 error_val =  0.06981141010864916 W =  [[2.30982512]\n",
      " [1.08248812]] , b =  [-27.19718335]\n",
      "step =  84400 error_val =  0.06949708958159806 W =  [[2.3125031 ]\n",
      " [1.08436104]] , b =  [-27.23250222]\n",
      "step =  84800 error_val =  0.06918557195867703 W =  [[2.31516927]\n",
      " [1.08622532]] , b =  [-27.26766321]\n",
      "step =  85200 error_val =  0.068876820061401 W =  [[2.31782374]\n",
      " [1.08808105]] , b =  [-27.30266773]\n",
      "step =  85600 error_val =  0.06857079736390506 W =  [[2.32046661]\n",
      " [1.08992831]] , b =  [-27.33751714]\n",
      "step =  86000 error_val =  0.06826746797870689 W =  [[2.32309798]\n",
      " [1.09176717]] , b =  [-27.37221281]\n",
      "step =  86400 error_val =  0.067966796642886 W =  [[2.32571794]\n",
      " [1.09359771]] , b =  [-27.40675609]\n",
      "step =  86800 error_val =  0.0676687487045754 W =  [[2.32832661]\n",
      " [1.09542   ]] , b =  [-27.44114829]\n",
      "step =  87200 error_val =  0.06737329010980263 W =  [[2.33092406]\n",
      " [1.09723412]] , b =  [-27.47539073]\n",
      "step =  87600 error_val =  0.06708038738970679 W =  [[2.33351041]\n",
      " [1.09904014]] , b =  [-27.50948469]\n",
      "step =  88000 error_val =  0.06679000764802971 W =  [[2.33608573]\n",
      " [1.10083812]] , b =  [-27.54343146]\n",
      "step =  88400 error_val =  0.06650211854896264 W =  [[2.33865013]\n",
      " [1.10262815]] , b =  [-27.57723229]\n",
      "step =  88800 error_val =  0.06621668830530313 W =  [[2.3412037 ]\n",
      " [1.10441029]] , b =  [-27.61088841]\n",
      "step =  89200 error_val =  0.06593368566688036 W =  [[2.34374652]\n",
      " [1.10618461]] , b =  [-27.64440107]\n",
      "step =  89600 error_val =  0.06565307990929128 W =  [[2.3462787 ]\n",
      " [1.10795117]] , b =  [-27.67777147]\n",
      "step =  90000 error_val =  0.06537484082292251 W =  [[2.3488003 ]\n",
      " [1.10971005]] , b =  [-27.7110008]\n",
      "step =  90400 error_val =  0.06509893870223184 W =  [[2.35131143]\n",
      " [1.11146131]] , b =  [-27.74409026]\n",
      "step =  90800 error_val =  0.06482534433531097 W =  [[2.35381217]\n",
      " [1.11320502]] , b =  [-27.77704099]\n",
      "step =  91200 error_val =  0.06455402899369685 W =  [[2.3563026 ]\n",
      " [1.11494124]] , b =  [-27.80985416]\n",
      "step =  91600 error_val =  0.0642849644224285 W =  [[2.35878281]\n",
      " [1.11667003]] , b =  [-27.8425309]\n",
      "step =  92000 error_val =  0.06401812283035299 W =  [[2.36125288]\n",
      " [1.11839146]] , b =  [-27.87507234]\n",
      "step =  92400 error_val =  0.06375347688068407 W =  [[2.3637129 ]\n",
      " [1.12010558]] , b =  [-27.90747958]\n",
      "step =  92800 error_val =  0.0634909996817553 W =  [[2.36616293]\n",
      " [1.12181247]] , b =  [-27.93975373]\n",
      "step =  93200 error_val =  0.06323066477804015 W =  [[2.36860308]\n",
      " [1.12351218]] , b =  [-27.97189586]\n",
      "step =  93600 error_val =  0.06297244614135171 W =  [[2.3710334 ]\n",
      " [1.12520477]] , b =  [-28.00390705]\n",
      "step =  94000 error_val =  0.06271631816226565 W =  [[2.37345399]\n",
      " [1.1268903 ]] , b =  [-28.03578834]\n",
      "step =  94400 error_val =  0.06246225564176322 W =  [[2.37586491]\n",
      " [1.12856883]] , b =  [-28.0675408]\n",
      "step =  94800 error_val =  0.06221023378305862 W =  [[2.37826625]\n",
      " [1.13024041]] , b =  [-28.09916544]\n",
      "step =  95200 error_val =  0.061960228183610515 W =  [[2.38065808]\n",
      " [1.1319051 ]] , b =  [-28.13066329]\n",
      "step =  95600 error_val =  0.061712214827355304 W =  [[2.38304048]\n",
      " [1.13356297]] , b =  [-28.16203536]\n",
      "step =  96000 error_val =  0.06146617007708518 W =  [[2.38541352]\n",
      " [1.13521406]] , b =  [-28.19328263]\n",
      "step =  96400 error_val =  0.061222070667034605 W =  [[2.38777726]\n",
      " [1.13685843]] , b =  [-28.2244061]\n",
      "step =  96800 error_val =  0.06097989369562606 W =  [[2.39013179]\n",
      " [1.13849613]] , b =  [-28.25540673]\n",
      "step =  97200 error_val =  0.060739616618383736 W =  [[2.39247718]\n",
      " [1.14012722]] , b =  [-28.2862855]\n",
      "step =  97600 error_val =  0.06050121724102791 W =  [[2.39481349]\n",
      " [1.14175175]] , b =  [-28.31704333]\n",
      "step =  98000 error_val =  0.0602646737126953 W =  [[2.3971408 ]\n",
      " [1.14336977]] , b =  [-28.34768119]\n",
      "step =  98400 error_val =  0.06002996451936048 W =  [[2.39945917]\n",
      " [1.14498133]] , b =  [-28.37819998]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step =  98800 error_val =  0.059797068477360014 W =  [[2.40176867]\n",
      " [1.14658649]] , b =  [-28.40860064]\n",
      "step =  99200 error_val =  0.05956596472711916 W =  [[2.40406937]\n",
      " [1.1481853 ]] , b =  [-28.43888406]\n",
      "step =  99600 error_val =  0.0593366327269661 W =  [[2.40636134]\n",
      " [1.1497778 ]] , b =  [-28.46905114]\n",
      "step =  100000 error_val =  0.05910905224711362 W =  [[2.40864464]\n",
      " [1.15136404]] , b =  [-28.49910278]\n"
     ]
    }
   ],
   "source": [
    "# 다변량 예제\n",
    "import numpy as np\n",
    "\n",
    "x_data = np.array([ [2, 4], [4, 11], [6, 6], [8, 5], [10, 7], [12, 16], [14, 8], [16, 3], [18, 7] ])\n",
    "t_data = np.array([0, 0, 0, 0, 1, 1, 1, 1, 1]).reshape(9, 1)\n",
    "\n",
    "W = np.random.rand(2,1)\n",
    "b = np.random.rand(1)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def loss_func(x, t):\n",
    "    delta = 1e-7 # log 내 0이 들어가면 무한대로 갈 수 있기 때문에 이를 방지하기 위함\n",
    "    z = np.dot(x, W) + b\n",
    "    y = sigmoid(z)\n",
    "    # cross-entropy\n",
    "    return -np.sum(t*np.log(y+delta) + (1-t)*np.log((1-y)+delta))\n",
    "\n",
    "def numerical_derivative(f, x):\n",
    "    delta_x = 1e-4\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    \n",
    "    while not it.finished:\n",
    "        idx = it.multi_index\n",
    "        tmp_val = x[idx]\n",
    "        \n",
    "        x[idx] = float(tmp_val) + delta_x\n",
    "        fx1 = f(x)\n",
    "        \n",
    "        x[idx] = tmp_val - delta_x\n",
    "        fx2 = f(x)\n",
    "        \n",
    "        grad[idx] = (fx1-fx2) / (2*delta_x)\n",
    "        \n",
    "        x[idx] = tmp_val\n",
    "        it. iternext()\n",
    "    \n",
    "    return grad\n",
    "\n",
    "def error_val(x, t):\n",
    "    delta = 1e-7 # log 내 0이 들어가면 무한대로 갈 수 있기 때문에 이를 방지하기 위함\n",
    "    z = np.dot(x, W) + b\n",
    "    y = sigmoid(z)\n",
    "    # cross-entropy\n",
    "    return -np.sum(t*np.log(y+delta) + (1-t)*np.log((1-y)+delta))\n",
    "\n",
    "def predict(x):\n",
    "    z = np.dot(x, W) + b\n",
    "    y = sigmoid(z)\n",
    "    \n",
    "    if y > 0.5:\n",
    "        result = 1 # True\n",
    "    else:\n",
    "        result = 0 # False\n",
    "        \n",
    "    return y, result\n",
    "\n",
    "learning_rate = 1e-2 #  손실함수값이 발산하는 경우 바꾸어서 실행\n",
    "f = lambda x : loss_func(x_data, t_data) # f(x) = loss_func(x_data, t_data)\n",
    "print(\"init error val = \", error_val(x_data, t_data), \"init W = \", W, \"\\n\", \", b = \", b)\n",
    "\n",
    "for step in range(100001):\n",
    "    W -= learning_rate * numerical_derivative(f, W)\n",
    "    b -= learning_rate * numerical_derivative(f, b)\n",
    "    \n",
    "    if (step%1000 == 0):\n",
    "        print(\"step = \", step, \"error_val = \", error_val(x_data, t_data), \"W = \", W, \", b = \", b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.59980025]), 1)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict([12,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# W1 = 2.4 , W2 = 1.15 -> x1에 대한 가중치가 더 크다? -> x1의 영향이 더 크다!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 논리게이트: and, or, nand, xor\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# sigmoid 함수\n",
    "def sigmoid(x):\n",
    "    return 1 / (1+np.exp(-x))\n",
    "\n",
    "# 수치미분 함수\n",
    "def numerical_derivative(f, x):\n",
    "    delta_x = 1e-4 # 0.0001\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    \n",
    "    while not it.finished:\n",
    "        idx = it.multi_index        \n",
    "        tmp_val = x[idx]\n",
    "        x[idx] = float(tmp_val) + delta_x\n",
    "        fx1 = f(x) # f(x+delta_x)\n",
    "        \n",
    "        x[idx] = tmp_val - delta_x \n",
    "        fx2 = f(x) # f(x-delta_x)\n",
    "        grad[idx] = (fx1 - fx2) / (2*delta_x)\n",
    "        \n",
    "        x[idx] = tmp_val \n",
    "        it.iternext()   \n",
    "        \n",
    "    return grad\n",
    "\n",
    "# LogicGate class (__init__, __loss_func, error_val)\n",
    "class LogicGate:\n",
    "    \n",
    "    def __init__(self, gate_name, xdata, tdata): # xdata, tdata => numpy.array(...)\n",
    "        self.name = gate_name\n",
    "        \n",
    "        # 입력데이터, 정답데이터 초기화\n",
    "        self.__xdata = xdata.reshape(4,2) # 입력데이터는 (0,0) (0,1) (1,0) (1,1) 총 4가지\n",
    "        self.__tdata = tdata.reshape(4,1)\n",
    "        \n",
    "        # 가중치 W, 바이어스 b 초기화\n",
    "        self.__W = np.random.rand(2,1) # weight, 2x1 matrix\n",
    "        self.__b = np.random.rand(1)\n",
    "        \n",
    "        # 학습율 초기화\n",
    "        self.__learning_rate = 1e-2 # 발산할 경우 더 작게 설정\n",
    "        \n",
    "        # 멤버 변수들이 모두 프라이빗으로 선언되어 있음. 왜? 파이썬 클래스에서 멤버 변수는 기본적으로 퍼블릭이기 때문에 외부에서 쉽게 접근해서 변경할 수 있기 때문에\n",
    "        \n",
    "    # 손실함수\n",
    "    def __loss_func(self):\n",
    "        \n",
    "        delta = 1e-7 # log 무한대 발산 방지\n",
    "        \n",
    "        z = np.dot(self.__xdata, self.__W) + self.__b\n",
    "        y = sigmoid(z)\n",
    "        \n",
    "        # cross-entropy\n",
    "        return -np.sum( self.__tdata*np.log(y + delta) + (1-self.__tdata)*np.log((1-y) + delta))\n",
    "    \n",
    "    # 손실값 계산\n",
    "    def error_val(self):\n",
    "        \n",
    "        delta = 1e-7 # log 무한대 발산 방지\n",
    "        \n",
    "        z = np.dot(self.__xdata, self.__W) + self.__b\n",
    "        y = sigmoid(z)\n",
    "        \n",
    "        # cross-entropy\n",
    "        return -np.sum( self.__tdata*np.log(y + delta) + (1-self.__tdata)*np.log((1-y) + delta))\n",
    "    \n",
    "    # 수치미분을 이용하여 손실함수가 최소가 될 때까지 학습하는 함수\n",
    "    def train(self):\n",
    "        \n",
    "        f = lambda x : self.__loss_func()\n",
    "        \n",
    "        print(\"init error val = \", self.error_val())\n",
    "        \n",
    "        for step in range(8001):\n",
    "            self.__W -= self.__learning_rate * numerical_derivative(f, self.__W)\n",
    "            self.__b -= self.__learning_rate * numerical_derivative(f, self.__b)\n",
    "            \n",
    "            if (step%400 == 0):\n",
    "                print(\"step = \", step, \"error value = \", self.error_val())\n",
    "                \n",
    "    # 미래 값 예측\n",
    "    def predict(self, input_data):\n",
    "        \n",
    "        z = np.dot(input_data, self.__W) + self.__b\n",
    "        y = sigmoid(z)\n",
    "        \n",
    "        if y > 0.5 :\n",
    "            result = 1 # True\n",
    "        else:\n",
    "            result = 0 # False\n",
    "        \n",
    "        return y, result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init error val =  4.978762689538606\n",
      "step =  0 error value =  4.9151081014042965\n",
      "step =  400 error value =  1.5407822861210194\n",
      "step =  800 error value =  1.1439455524679079\n",
      "step =  1200 error value =  0.9194255809430008\n",
      "step =  1600 error value =  0.7713751254703193\n",
      "step =  2000 error value =  0.6650092371209055\n",
      "step =  2400 error value =  0.5843817226427886\n",
      "step =  2800 error value =  0.5209720557245154\n",
      "step =  3200 error value =  0.46973333262513267\n",
      "step =  3600 error value =  0.4274514311333886\n",
      "step =  4000 error value =  0.3919673622374862\n",
      "step =  4400 error value =  0.3617705216875883\n",
      "step =  4800 error value =  0.3357693026775475\n",
      "step =  5200 error value =  0.31315388759987994\n",
      "step =  5600 error value =  0.2933102218658096\n",
      "step =  6000 error value =  0.2757639658433629\n",
      "step =  6400 error value =  0.26014279343699864\n",
      "step =  6800 error value =  0.24615033792242536\n",
      "step =  7200 error value =  0.2335477650508046\n",
      "step =  7600 error value =  0.22214047572534318\n",
      "step =  8000 error value =  0.2117683391921832\n"
     ]
    }
   ],
   "source": [
    "xdata = np.array([ [0, 0], [0, 1], [1, 0], [1, 1] ])\n",
    "tdata = np.array([0, 0, 0, 1])\n",
    "\n",
    "AND_obj = LogicGate(\"AND_GATE\", xdata, tdata)\n",
    "\n",
    "AND_obj.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AND_GATE \n",
      "\n",
      "[0 0]  =  0 \n",
      "\n",
      "[0 1]  =  0 \n",
      "\n",
      "[1 0]  =  0 \n",
      "\n",
      "[1 1]  =  1 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# AND Gate prediction\n",
    "print(AND_obj.name, \"\\n\")\n",
    "\n",
    "test_data = np.array([ [0, 0], [0, 1], [1, 0], [1, 1] ])\n",
    "\n",
    "for input_data in test_data:\n",
    "    (sigmoid_val, logical_val) = AND_obj.predict(input_data) \n",
    "    print(input_data, \" = \", logical_val, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init error val =  1.85843693562649\n",
      "step =  0 error value =  1.8530142415772646\n",
      "step =  400 error value =  1.1102382686890384\n",
      "step =  800 error value =  0.8038052961878829\n",
      "step =  1200 error value =  0.6241612176273524\n",
      "step =  1600 error value =  0.5071785068196584\n",
      "step =  2000 error value =  0.42543124192220727\n",
      "step =  2400 error value =  0.3653628195657569\n",
      "step =  2800 error value =  0.31952347315554214\n",
      "step =  3200 error value =  0.2834917269037545\n",
      "step =  3600 error value =  0.2544855815223688\n",
      "step =  4000 error value =  0.23067199236635116\n",
      "step =  4400 error value =  0.21079698857327742\n",
      "step =  4800 error value =  0.19397551146453776\n",
      "step =  5200 error value =  0.17956612209919356\n",
      "step =  5600 error value =  0.16709324591981894\n",
      "step =  6000 error value =  0.15619724243834676\n",
      "step =  6400 error value =  0.14660138374674905\n",
      "step =  6800 error value =  0.13808944636011503\n",
      "step =  7200 error value =  0.1304901561385251\n",
      "step =  7600 error value =  0.12366617031240462\n",
      "step =  8000 error value =  0.11750613094674231\n"
     ]
    }
   ],
   "source": [
    "xdata = np.array([ [0, 0], [0, 1], [1, 0], [1, 1] ])\n",
    "tdata = np.array([0, 1, 1, 1])\n",
    "\n",
    "OR_obj = LogicGate(\"OR_GATE\", xdata, tdata)\n",
    "\n",
    "OR_obj.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OR_GATE \n",
      "\n",
      "[0 0]  =  0 \n",
      "\n",
      "[0 1]  =  1 \n",
      "\n",
      "[1 0]  =  1 \n",
      "\n",
      "[1 1]  =  1 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# OR Gate prediction\n",
    "print(OR_obj.name, \"\\n\")\n",
    "\n",
    "test_data = np.array([ [0, 0], [0, 1], [1, 0], [1, 1] ])\n",
    "\n",
    "for input_data in test_data:\n",
    "    (sigmoid_val, logical_val) = OR_obj.predict(input_data) \n",
    "    print(input_data, \" = \", logical_val, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init error val =  3.1887909868528874\n",
      "step =  0 error value =  3.18236575383791\n",
      "step =  400 error value =  1.7793576467603978\n",
      "step =  800 error value =  1.2607003381011923\n",
      "step =  1200 error value =  0.9904641745342762\n",
      "step =  1600 error value =  0.8200122693133451\n",
      "step =  2000 error value =  0.7007276097214161\n",
      "step =  2400 error value =  0.6118478650044574\n",
      "step =  2800 error value =  0.5427939274201365\n",
      "step =  3200 error value =  0.4875027208742415\n",
      "step =  3600 error value =  0.4422037625289072\n",
      "step =  4000 error value =  0.4044091577228043\n",
      "step =  4400 error value =  0.3724021489326962\n",
      "step =  4800 error value =  0.3449558573666343\n",
      "step =  5200 error value =  0.321168376712088\n",
      "step =  5600 error value =  0.30036103801643554\n",
      "step =  6000 error value =  0.2820130027646671\n",
      "step =  6400 error value =  0.2657177535050853\n",
      "step =  6800 error value =  0.25115330950413933\n",
      "step =  7200 error value =  0.238061333005733\n",
      "step =  7600 error value =  0.2262321588274404\n",
      "step =  8000 error value =  0.21549386755355848\n"
     ]
    }
   ],
   "source": [
    "xdata = np.array([ [0, 0], [0, 1], [1, 0], [1, 1] ])\n",
    "tdata = np.array([1, 1, 1, 0])\n",
    "\n",
    "NAND_obj = LogicGate(\"NAND_GATE\", xdata, tdata)\n",
    "\n",
    "NAND_obj.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NAND_GATE \n",
      "\n",
      "[0 0]  =  1 \n",
      "\n",
      "[0 1]  =  1 \n",
      "\n",
      "[1 0]  =  1 \n",
      "\n",
      "[1 1]  =  0 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# NAND Gate prediction\n",
    "print(NAND_obj.name, \"\\n\")\n",
    "\n",
    "test_data = np.array([ [0, 0], [0, 1], [1, 0], [1, 1] ])\n",
    "\n",
    "for input_data in test_data:\n",
    "    (sigmoid_val, logical_val) = NAND_obj.predict(input_data) \n",
    "    print(input_data, \" = \", logical_val, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init error val =  3.3779006225322235\n",
      "step =  0 error value =  3.3631967046166293\n",
      "step =  400 error value =  2.7795961946555248\n",
      "step =  800 error value =  2.7736498057684758\n",
      "step =  1200 error value =  2.772763556519934\n",
      "step =  1600 error value =  2.7726207703171952\n",
      "step =  2000 error value =  2.7725949462965946\n",
      "step =  2400 error value =  2.7725896044289575\n",
      "step =  2800 error value =  2.772588357351388\n",
      "step =  3200 error value =  2.7725880399363803\n",
      "step =  3600 error value =  2.772587954838477\n",
      "step =  4000 error value =  2.7725879313765445\n",
      "step =  4400 error value =  2.7725879248155643\n",
      "step =  4800 error value =  2.772587922968005\n",
      "step =  5200 error value =  2.7725879224459815\n",
      "step =  5600 error value =  2.7725879222982464\n",
      "step =  6000 error value =  2.7725879222564047\n",
      "step =  6400 error value =  2.7725879222445493\n",
      "step =  6800 error value =  2.77258792224119\n",
      "step =  7200 error value =  2.772587922240238\n",
      "step =  7600 error value =  2.772587922239968\n",
      "step =  8000 error value =  2.7725879222398917\n"
     ]
    }
   ],
   "source": [
    "xdata = np.array([ [0, 0], [0, 1], [1, 0], [1, 1] ])\n",
    "tdata = np.array([0, 1, 1, 0])\n",
    "\n",
    "\n",
    "XOR_obj = LogicGate(\"XOR_GATE\", xdata, tdata)\n",
    "\n",
    "# XOR Gate 를 보면, 손실함수 값이 2.7 근처에서 더 이상 감소하지 않는것을 볼 수 있음\n",
    "XOR_obj.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XOR_GATE \n",
      "\n",
      "[0 0]  =  0 \n",
      "\n",
      "[0 1]  =  0 \n",
      "\n",
      "[1 0]  =  0 \n",
      "\n",
      "[1 1]  =  1 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# XOR Gate prediction => 예측이 되지 않음\n",
    "print(XOR_obj.name, \"\\n\")\n",
    "\n",
    "test_data = np.array([ [0, 0], [0, 1], [1, 0], [1, 1] ])\n",
    "\n",
    "for input_data in test_data:\n",
    "    (sigmoid_val, logical_val) = XOR_obj.predict(input_data) \n",
    "    print(input_data, \" = \", logical_val, \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0]  =  0\n",
      "\n",
      "[0 1]  =  1\n",
      "\n",
      "[1 0]  =  1\n",
      "\n",
      "[1 1]  =  0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# XOR 을 NAND + OR => AND 조합으로 계산\n",
    "import numpy as np\n",
    "\n",
    "input_data = np.array([[0,0],[0,1],[1,0],[1,1]])\n",
    "\n",
    "s1 = [] # NAND 출력\n",
    "s2 = [] # OR 출력\n",
    "\n",
    "new_input_data = [] # AND 입력\n",
    "final_output = [] # AND 출력\n",
    "\n",
    "for index in range(len(input_data)):\n",
    "    \n",
    "    s1 = NAND_obj.predict(input_data[index])\n",
    "    s2 = OR_obj.predict(input_data[index])\n",
    "    \n",
    "    new_input_data.append(s1[-1])\n",
    "    new_input_data.append(s2[-1])\n",
    "    \n",
    "    (sigmoid_val, logical_val) = AND_obj.predict(np.array(new_input_data))\n",
    "    \n",
    "    final_output.append(logical_val) # AND 출력, 즉 XOR 출력\n",
    "    new_input_data = [] # AND 입력 초기화\n",
    "    \n",
    "for index in range(len(input_data)):\n",
    "    print(input_data[index], \" = \", final_output[index], end='')\n",
    "    print(\"\\n\")\n",
    "\n",
    "# 머신러닝 XOR 문제는 다양한 Gate 조합인 Multi-Layer로 해결할 수 있음\n",
    "# 이는 신경망 기반의 딥러닝 핵심 아이디어"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
