{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 지도학습 Linear Regression\n",
    "# 트레이닝 데이터의 특성을 가장 잘 표현할 수 있는 가중치 W(기울기), 바이어스 b(y절편)을 찾는 것이 중요\n",
    "# 손실함수: 최소제곱법인듯 (오차제곱의 합이 최소)\n",
    "# 경사하강법: 손실함수가 최소가 되는 지점을 찾기\n",
    "# 경사하강 W값 구하기: 편미분값이 양수일 때에는 왼쪽으로 이동시켜야(기울기를 감소시켜야) 최소값 찾음, 음수일 때는 반대"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파이썬에서 구현하는 순서\n",
    "# 1. 슬라이싱 또는 list comprehension 등을 이용하여 입력 x와 정답 t를 numpy 데이터형으로 분리\n",
    "# 2. y = Wx + b : W= numpy.random.rand(...) , b = numpy.random.rand(...)\n",
    "# 3. 손실함수: def loss_func(...): y = numpy.dot(X, W) + b return(numpy.sum((t-y)**2))/(len(x))\n",
    "# 4. 학습률 a : learngin_rate = 1e-3, 1e-4, 1e-5 ...\n",
    "# 5. 가중치 W, 바이어스 b: f = lambda x: loss_func(...)\n",
    "#                          for step in range(6000): (6000은 임의값) W -=learning_rate*numerical_derivative(f,W) b -= learning_rate*numerical_derivative(f, b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init error val =  0.5464844110101867 init W =  [[0.81081383]] \n",
      " , b =  [0.87842747]\n",
      "step =  0 error_val =  0.3218838512608027 W =  [[0.85972914]] , b =  [0.88927517]\n",
      "step =  400 error_val =  6.685404585592941e-05 W =  [[1.00530958]] , b =  [0.98083541]\n",
      "step =  800 error_val =  4.265683364911467e-06 W =  [[1.00134119]] , b =  [0.99515906]\n",
      "step =  1200 error_val =  2.721758172855031e-07 W =  [[1.00033878]] , b =  [0.99877719]\n",
      "step =  1600 error_val =  1.73664262388718e-08 W =  [[1.00008558]] , b =  [0.99969112]\n",
      "step =  2000 error_val =  1.1080806638921059e-09 W =  [[1.00002162]] , b =  [0.99992198]\n",
      "step =  2400 error_val =  7.07020972967071e-11 W =  [[1.00000546]] , b =  [0.99998029]\n",
      "step =  2800 error_val =  4.51121179621456e-12 W =  [[1.00000138]] , b =  [0.99999502]\n",
      "step =  3200 error_val =  2.878419828286536e-13 W =  [[1.00000035]] , b =  [0.99999874]\n",
      "step =  3600 error_val =  1.836602024455449e-14 W =  [[1.00000009]] , b =  [0.99999968]\n",
      "step =  4000 error_val =  1.1718606882513085e-15 W =  [[1.00000002]] , b =  [0.99999992]\n",
      "step =  4400 error_val =  7.477164198580029e-17 W =  [[1.00000001]] , b =  [0.99999998]\n",
      "step =  4800 error_val =  4.770871646254218e-18 W =  [[1.]] , b =  [0.99999999]\n",
      "step =  5200 error_val =  3.0440986845884404e-19 W =  [[1.]] , b =  [1.]\n",
      "step =  5600 error_val =  1.9423192992151824e-20 W =  [[1.]] , b =  [1.]\n",
      "step =  6000 error_val =  1.2393118448251061e-21 W =  [[1.]] , b =  [1.]\n",
      "step =  6400 error_val =  7.907586919609596e-23 W =  [[1.]] , b =  [1.]\n",
      "step =  6800 error_val =  5.046005459902242e-24 W =  [[1.]] , b =  [1.]\n",
      "step =  7200 error_val =  3.231634173693008e-25 W =  [[1.]] , b =  [1.]\n",
      "step =  7600 error_val =  2.0674821591439627e-26 W =  [[1.]] , b =  [1.]\n",
      "step =  8000 error_val =  1.3142817111434667e-27 W =  [[1.]] , b =  [1.]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "x_data = np.array([1,2,3,4,5]).reshape(5,1) # 입력데이터\n",
    "t_data = np.array([2,3,4,5,6]).reshape(5,1) # 정답데이터\n",
    "\n",
    "# raw_data = [[1,2],[2,3],[3,4],[4,5],[5,6]]\n",
    "# x_data = [x[0] for x in raw_data]\n",
    "\n",
    "W = np.random.rand(1,1) # W.shape = (1,1) 0~1 사이의 랜덤값은 똑같은데, 형태가 다름\n",
    "b = np.random.rand(1) # b.shape = (1,)\n",
    "\n",
    "def loss_func(x, t):\n",
    "    y = np.dot(x,W) + b\n",
    "    \n",
    "    return (np.sum((t-y)**2))/(len(x))\n",
    "\n",
    "def numerical_derivative(f, x):\n",
    "    delta_x = 1e-4\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    \n",
    "    while not it.finished:\n",
    "        idx = it.multi_index\n",
    "        tmp_val = x[idx]\n",
    "        \n",
    "        x[idx] = float(tmp_val) + delta_x\n",
    "        fx1 = f(x)\n",
    "        \n",
    "        x[idx] = tmp_val - delta_x\n",
    "        fx2 = f(x)\n",
    "        \n",
    "        grad[idx] = (fx1-fx2) / (2*delta_x)\n",
    "        \n",
    "        x[idx] = tmp_val\n",
    "        it. iternext()\n",
    "    \n",
    "    return grad\n",
    "\n",
    "# 손실함수 값 계산 함수\n",
    "# 입력변수 x, t 는 np type\n",
    "def error_val(x, t):\n",
    "    y = np.dot(x,W) + b\n",
    "    \n",
    "    return (np.sum((t-y)**2))/(len(x))\n",
    "\n",
    "# 학습률 초기화 및 손실함수가 최소가 될 때까지 W, B 업데이트\n",
    "learning_rate = 1e-2\n",
    "f = lambda x : loss_func(x_data, t_data) # f(x) = loss_func(x_data, t_data)\n",
    "print(\"init error val = \", error_val(x_data, t_data), \"init W = \", W, \"\\n\", \", b = \", b)\n",
    "\n",
    "for step in range(8001):\n",
    "    W -= learning_rate * numerical_derivative(f, W)\n",
    "    b -= learning_rate * numerical_derivative(f, b)\n",
    "    \n",
    "    if (step%400 == 0):\n",
    "        print(\"step = \", step, \"error_val = \", error_val(x_data, t_data), \"W = \", W, \", b = \", b)\n",
    "\n",
    "# 학습을 마친 후 임의의 데이터에 대해 미래값 예측하는 함수\n",
    "# 입력변수 x: np type\n",
    "def predict(x):\n",
    "    y = np.dot(x,W) + b\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[44.]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict(43)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init error val =  7414.706218746868 init W =  [[0.27812434]\n",
      " [0.03656099]\n",
      " [0.63612446]] \n",
      " , b =  [0.18933524]\n",
      "step =  0 error_val =  2746.2279148488437 W =  [[0.41558402]\n",
      " [0.17488004]\n",
      " [0.77766738]] , b =  [0.19037002]\n",
      "step =  400 error_val =  7.2650393926551216 W =  [[0.60296615]\n",
      " [0.40248978]\n",
      " [1.00968813]] , b =  [0.19143187]\n",
      "step =  800 error_val =  7.059799704019952 W =  [[0.57945984]\n",
      " [0.41436566]\n",
      " [1.0209345 ]] , b =  [0.19087149]\n",
      "step =  1200 error_val =  6.891744676561907 W =  [[0.55819172]\n",
      " [0.42515061]\n",
      " [1.03107191]] , b =  [0.19029781]\n",
      "step =  1600 error_val =  6.754130984540842 W =  [[0.5389485 ]\n",
      " [0.43494212]\n",
      " [1.04021234]] , b =  [0.18971216]\n",
      "step =  2000 error_val =  6.641438883953415 W =  [[0.52153726]\n",
      " [0.44382943]\n",
      " [1.04845604]] , b =  [0.18911571]\n",
      "step =  2400 error_val =  6.549149983694576 W =  [[0.5057835 ]\n",
      " [0.45189416]\n",
      " [1.05589287]] , b =  [0.18850953]\n",
      "step =  2800 error_val =  6.473565440727598 W =  [[0.49152933]\n",
      " [0.45921087]\n",
      " [1.06260338]] , b =  [0.18789459]\n",
      "step =  3200 error_val =  6.411657207380498 W =  [[0.47863194]\n",
      " [0.46584763]\n",
      " [1.06865984]] , b =  [0.18727174]\n",
      "step =  3600 error_val =  6.360946308811021 W =  [[0.4669621 ]\n",
      " [0.47186655]\n",
      " [1.07412713]] , b =  [0.18664178]\n",
      "step =  4000 error_val =  6.319403229760046 W =  [[0.45640294]\n",
      " [0.47732423]\n",
      " [1.07906353]] , b =  [0.18600539]\n",
      "step =  4400 error_val =  6.285366388302997 W =  [[0.4468487 ]\n",
      " [0.48227225]\n",
      " [1.0835214 ]] , b =  [0.18536321]\n",
      "step =  4800 error_val =  6.257475407995697 W =  [[0.4382037 ]\n",
      " [0.48675757]\n",
      " [1.08754783]] , b =  [0.18471582]\n",
      "step =  5200 error_val =  6.234616499094358 W =  [[0.43038138]\n",
      " [0.49082295]\n",
      " [1.09118516]] , b =  [0.18406373]\n",
      "step =  5600 error_val =  6.215877749206561 W =  [[0.42330343]\n",
      " [0.49450727]\n",
      " [1.09447149]] , b =  [0.18340741]\n",
      "step =  6000 error_val =  6.200512523962535 W =  [[0.41689899]\n",
      " [0.49784588]\n",
      " [1.09744114]] , b =  [0.18274726]\n",
      "step =  6400 error_val =  6.187909505506641 W =  [[0.41110395]\n",
      " [0.50087093]\n",
      " [1.10012499]] , b =  [0.18208368]\n",
      "step =  6800 error_val =  6.177568164181512 W =  [[0.40586032]\n",
      " [0.50361162]\n",
      " [1.10255088]] , b =  [0.181417]\n",
      "step =  7200 error_val =  6.1690786776198765 W =  [[0.4011156 ]\n",
      " [0.50609448]\n",
      " [1.10474386]] , b =  [0.18074753]\n",
      "step =  7600 error_val =  6.162105490477851 W =  [[0.39682233]\n",
      " [0.5083436 ]\n",
      " [1.10672656]] , b =  [0.18007555]\n",
      "step =  8000 error_val =  6.1563738545032844 W =  [[0.39293753]\n",
      " [0.51038084]\n",
      " [1.10851935]] , b =  [0.17940131]\n"
     ]
    }
   ],
   "source": [
    "# multi-variable regression\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "loaded_data = np.loadtxt('./test_score.txt', delimiter = ',', dtype = np.float32)\n",
    "x_data = loaded_data[:, 0:-1]\n",
    "# 다른방법?\n",
    "# x_data2 = [ x[0:-1] for x in loaded_data]\n",
    "# print(x_data2)\n",
    "t_data = loaded_data[:, [-1]]\n",
    "\n",
    "W = np.random.rand(3,1) # 3x1 행렬\n",
    "b = np.random.rand(1)\n",
    "\n",
    "def loss_func(x, t):\n",
    "    y = np.dot(x,W) + b\n",
    "    \n",
    "    return (np.sum((t-y)**2))/(len(x))\n",
    "\n",
    "def numerical_derivative(f, x):\n",
    "    delta_x = 1e-4\n",
    "    grad = np.zeros_like(x)\n",
    "    \n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    \n",
    "    while not it.finished:\n",
    "        idx = it.multi_index\n",
    "        tmp_val = x[idx]\n",
    "        \n",
    "        x[idx] = float(tmp_val) + delta_x\n",
    "        fx1 = f(x)\n",
    "        \n",
    "        x[idx] = tmp_val - delta_x\n",
    "        fx2 = f(x)\n",
    "        \n",
    "        grad[idx] = (fx1-fx2) / (2*delta_x)\n",
    "        \n",
    "        x[idx] = tmp_val\n",
    "        it. iternext()\n",
    "    \n",
    "    return grad\n",
    "\n",
    "# 손실함수 값 계산 함수\n",
    "# 입력변수 x, t 는 np type\n",
    "def error_val(x, t):\n",
    "    y = np.dot(x,W) + b\n",
    "    \n",
    "    return (np.sum((t-y)**2))/(len(x))\n",
    "\n",
    "# 학습률 초기화 및 손실함수가 최소가 될 때까지 W, B 업데이트\n",
    "learning_rate = 1e-5 # 1e-2로 했을 때 오류 발생(값이 이상했음..) 이는 손실함수 값이 발산하기 때문\n",
    "f = lambda x : loss_func(x_data, t_data) # f(x) = loss_func(x_data, t_data)\n",
    "print(\"init error val = \", error_val(x_data, t_data), \"init W = \", W, \"\\n\", \", b = \", b)\n",
    "\n",
    "for step in range(8001):\n",
    "    W -= learning_rate * numerical_derivative(f, W)\n",
    "    b -= learning_rate * numerical_derivative(f, b)\n",
    "    \n",
    "    if (step%400 == 0):\n",
    "        print(\"step = \", step, \"error_val = \", error_val(x_data, t_data), \"W = \", W, \", b = \", b)\n",
    "\n",
    "# 학습을 마친 후 임의의 데이터에 대해 미래값 예측하는 함수\n",
    "# 입력변수 x: np type\n",
    "def predict(x):\n",
    "    y = np.dot(x,W) + b\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([179.28054374])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = np.array([100,98,81])\n",
    "predict(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
